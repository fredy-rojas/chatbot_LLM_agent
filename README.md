[Watch the video](https://www.youtube.com/watch?v=3CvPp7rMKX4)

I've been experimenting with Multi-Agent LLM Architectures (chatbot) and the Retrieval-Augmented Generation (RAG) technique to extract insights from analytics tools and provide more valuable information to end users. I believe the best way to truly understand a concept is by building something with. So, I did just thatüòé.

The idea is to create a chatbot that increases the sales conversion rate for a coffee shop by offering product recommendations that customers are more likely to purchase and equipping the chatbot with specialized sales pitch scripts designed to close more sales.

Main takeaways: 
- LLMs are inherently probabilistic, which is beneficial for handling multiple customer requests but makes it difficult to control their responses. This is where prompt engineering becomes crucial. I'm still experimenting with structuring better prompts. Here‚Äôs a useful resource I‚Äôve been using [prompt Engineeirng](https://www.promptingguide.ai/techniques)

- Generating application-level actions (e.g., adding items to the cart, sending order details to the counter) requires explicitly defining output formats in prompts, adding an additional agent to control output formats, and handling errors with the customer experience in mind. 

- Deploying the chatbot will require robust monitoring, including response logging, sentiment analysis to detect user frustration, and other strategies to maintain performance and user satisfaction.

Overall, this approach provides a human-like AI assistant that enhances user experience and saves time. While LLM-based architectures are still evolving, they already offer strong potential, and this is just the baselineüßê. It‚Äôs clear that these models will continue to improve rapidly in the near future.


________________
**Future Work:**

Before deployment, it is crucial to ensure that the Multi-Agent solution functions reliably. My next steps will focus on this goal through the following approaches:
- Reference-Based and Reference-Free Evaluation:
  - Using standard benchmarks and automated metrics to assess model outputs with and without ground-truth references.
- Developing an LLM Judge and Evaluating It:
    - Implementing a Large Language Model (LLM) as a judge to evaluate agent-generated responses and assessing the reliability of this judge.
- User Simulation Scenarios:
  - Creating simulated user interactions to test how agents respond to various real-world situations.
